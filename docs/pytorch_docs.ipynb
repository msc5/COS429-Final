{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import conv3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2d\n",
    "\n",
    "This is PyTorch's implementation of a 2D convolutional layer. Note that the layer exists as its own object. This seems counterintuitive, but it makes chaining layers together with different effects much easier. Doing so is commonly done by creating a super class which extends nn.Conv2d (for example) and implements a forward(self, x) method, indicating how an input to the layer (or layer block, etc.) is passed through.\n",
    "\n",
    "### This code excerpt does the following:\n",
    "- Generates a random tensor of size (batch_size, channel_size, h, w)\n",
    "- Pads tensor with (left, right, top, bot)\n",
    "- Convolves tensor with a convolutional layer of shape (in_channels, out_channels, (h, w)) (This means that the number of filters in the convolutional layer is equal to out_channels)\n",
    "\n",
    "### Note:\n",
    "- Calling nn.Conv2d returns a function. Calling that returned function equates to passing the input tensor through the filter, i.e. performing the convolution\n",
    "- Conv2d defaults to performing \"valid\" convolution, and as far as I know there is no easy way to change this to \"same\" or \"full\". One way to fix this would be to define a custom \"Conv2dAuto\" class which supercedes Conv2d and implements the desired padding. (See resnet_test.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 62, 62])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(64, 3, 64, 64)\n",
    "\n",
    "nn.Conv2d(3, 3, (3, 3), \n",
    "    stride=1,\n",
    "    padding=0\n",
    ")(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolution Block with auto padding (sets padding amount to half of kernel size)\n",
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # overrides padding parameter\n",
    "        self.padding = (self.kernel_size[0] // 2, self.kernel_size[1] // 2)\n",
    "        \n",
    "Conv2dAuto(3, 3, (3, 3), \n",
    "    stride=1,\n",
    "    padding=0   \n",
    ")(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Mapping (Skip Connections)\n",
    "\n",
    "- ResidualBlock extends nn.Module, which is meant to represent extensions or combinations of layers\n",
    "- When you initialize a ResidualBlock now, it also returns a function just like Conv2d\n",
    "    - For this to work, ResidualBlock must implement `__init__()` and `forward()`\n",
    "    - the `@property` decorator here is like using a getter or setter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(activation):\n",
    "    return nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]\n",
    "\n",
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "# Tried to simplify some things, idk if this works fully\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            activation='relu', \n",
    "            expansion=1, \n",
    "            downsampling=1, \n",
    "            conv=conv3x3, \n",
    "            *args, \n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.activate = activation_func(activation)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                self.in_channels,\n",
    "                self.expanded_channels,\n",
    "                kernel_size=1,\n",
    "                stride=self.downsampling,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.expanded_channels)\n",
    "        ) if self.should_apply_shortcut else None\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.out_channels, conv=self.conv, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut:\n",
    "            residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 64, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(64, 3, 64, 64)\n",
    "\n",
    "y = ResidualBlock(3, 3)(x)\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4853ae2d3524f344d9460ca4428ab3ecc7df6a6de34f9290d7b4bf6b49fd90b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('cos429': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
